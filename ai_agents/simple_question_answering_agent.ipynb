{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekWCqjtCyPap"
      },
      "source": [
        "# Simple Question-Answering Agent\n",
        "### Overview\n",
        "This notebook introduces a basic Question-Answering (QA) agent using LangChain and OpenAI's/Groq language model. The agent is designed to understand user queries and provide relevant, concise answers.\n",
        "\n",
        "### Motivation\n",
        "In the era of AI-driven interactions, creating a simple QA agent serves as a fundamental stepping stone towards more complex AI systems. This project aims to:\n",
        "\n",
        "- Demonstrate the basics of AI-driven question-answering\n",
        "- Introduce key concepts in building AI agents\n",
        "- Provide a foundation for more advanced agent architectures\n",
        "### Key Components\n",
        "- Language Model: Utilizes OpenAI's GPT model or Groq model for natural language understanding and generation.\n",
        "Prompt Template: Defines the structure and context for the agent's responses.\n",
        "LLMChain: Combines the language model and prompt template for streamlined processing.\n",
        "### Method Details\n",
        "1. Setup and Initialization\n",
        "- Import necessary libraries (LangChain, dotenv)\n",
        "- Load environment variables for API key management\n",
        "- Initialize the OpenAI/Groq language model\n",
        "2. Defining the Prompt Template\n",
        "- Create a template that instructs the AI on its role and response format\n",
        "- Use the PromptTemplate class to structure the input\n",
        "3. Creating the LLMChain\n",
        "- Combine the language model and prompt template into an LLMChain\n",
        "- This chain manages the flow from user input to AI response\n",
        "4. Implementing the Question-Answering Function\n",
        "- Define a function that takes a user question as input\n",
        "- Use the LLMChain to process the question and generate an answer\n",
        "5. User Interaction\n",
        "- In a Jupyter notebook environment, provide cells for:\n",
        "  - Example usage with a predefined question\n",
        "  - Interactive input for user questions\n",
        "\n",
        "### Conclusion\n",
        "This Simple Question-Answering Agent serves as an entry point into the world of AI agents. By understanding and implementing this basic model, you've laid the groundwork for more sophisticated systems. Future enhancements could include:\n",
        "\n",
        "- Adding memory to maintain context across multiple questions\n",
        "- Integrating external knowledge bases for more informed responses\n",
        "- Implementing more complex decision-making processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MX6DmRo1Mvu"
      },
      "source": [
        "##### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw_gY0p8y5Ip",
        "outputId": "f0c3ee43-2111-42df-8b27-2696ed790858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2.2/2.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain huggingface_hub openai  tiktoken python-dotenv\n",
        "!pip -q install sentence_transformers langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9d1b41b",
        "outputId": "f17a358f-b5c5-4256-a53f-cd0ced6b3c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0MZO1rz918_d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "# os.environ[\"OPEN_API_KEY\"] = userdata.get(\"OPEN_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUcMSXE49I6"
      },
      "source": [
        "##### Load environment variables and initalize the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f91c81b0"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(model_name = \"llama3-8b-8192\", max_tokens=1000, temperature=0)\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHvJAQAn5HGy"
      },
      "source": [
        "##### Define prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s1Ek4lX04tUz"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are a helpful assistant. Ypur task is to answer the user's query to the best of your knowledge.Give the output in precise and in short.\n",
        "User's question: {question}\n",
        "Please provide a clear and concise answer.\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-WjpnL5mBb"
      },
      "source": [
        "##### Create the LLM Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KnQblRD15pzL"
      },
      "outputs": [],
      "source": [
        "qa_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdzPY5i6520J"
      },
      "source": [
        "##### Define the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tL-zO0iT58mi"
      },
      "outputs": [],
      "source": [
        "def get_answer(question):\n",
        "  \"\"\"\n",
        "  Get an answer to the given user question using the QA chain\n",
        "  \"\"\"\n",
        "  input_variables = {\"question\": question}\n",
        "  response = qa_chain.invoke(input_variables).content\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nn3nLsE6A5i"
      },
      "source": [
        "##### Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U68LlXCw6FRy",
        "outputId": "473c51bd-4d84-41ac-c8e1-afae1cfeaa7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Who is the father of AI?\n",
            "Answer: The father of Artificial Intelligence (AI) is Alan Turing.\n"
          ]
        }
      ],
      "source": [
        "question = \"Who is the father of AI?\"\n",
        "answer = get_answer(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKfJC7ME6dmB"
      },
      "source": [
        "##### Interactive cell for user questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZfk7bvw6b3S",
        "outputId": "ab0adba2-dac8-4186-d8a4-285f94a9a329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: what is the best way to learn AI\n",
            "Answer: To learn AI, I recommend the following steps:\n",
            "\n",
            "1. **Start with the basics**: Understand the fundamentals of computer science, mathematics, and programming (Python, R, or Java).\n",
            "2. **Choose a specialization**: Focus on a specific area of AI, such as machine learning, deep learning, natural language processing, or computer vision.\n",
            "3. **Take online courses**: Utilize platforms like Coursera, edX, and Udemy, which offer a wide range of AI-related courses.\n",
            "4. **Practice with projects**: Apply theoretical knowledge to real-world projects, using datasets and libraries like TensorFlow, PyTorch, or scikit-learn.\n",
            "5. **Read research papers**: Stay updated with the latest research and advancements in AI by reading papers and articles on arXiv, ResearchGate, or Academia.edu.\n",
            "6. **Join online communities**: Participate in forums like Kaggle, Reddit (r/MachineLearning and r/AI), and Stack Overflow to connect with professionals and learn from their experiences.\n",
            "7. **Network with experts**: Attend conferences, meetups, or webinars to learn from industry leaders and experts in AI.\n",
            "\n",
            "Remember, learning AI is a continuous process. Stay curious, persistent, and patient, and you'll be well on your way to becoming an AI expert!\n"
          ]
        }
      ],
      "source": [
        "user_question = input(\"Enter your question: \")\n",
        "user_answer = get_answer(user_question)\n",
        "print(f\"Answer: {user_answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
