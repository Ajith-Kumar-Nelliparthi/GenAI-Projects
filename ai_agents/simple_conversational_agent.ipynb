{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekWCqjtCyPap"
      },
      "source": [
        "# Building a Conversational Agent With Context Awareness\n",
        "### Overview\n",
        "This notebook outlines the process of conversational agent that maintains context across multiple interactions.We'll use a modern AI framework to build an agent capable of engaging in more natural and coherent conversations.\n",
        "\n",
        "### Motivation\n",
        "Many simple chatbots lack the ability to maintain context, leading to disjointed and frustrating user experiences. This project aims to solve the problem by implementing a conversational agent that can remember and refer to previous parts of the conversation, enhancing the overall interaction quality.\n",
        "\n",
        "### Key Components\n",
        "1. Language Model: The core AI component that generates responses.\n",
        "2. Prompt Template: Defines the structure of our conversations.\n",
        "3. History Manager: Manages conversation history and context.\n",
        "4. Message Store: Stores the messages for each conversation session.\n",
        "\n",
        "### Method Details\n",
        "#### Setting up the environment\n",
        "Begin by setting up the necessary AI framework and ensuring access to a suitable language model. This forms the foundation of our conversational agent.\n",
        "\n",
        "#### Creating the Chat History Store\n",
        "Implement a system to manage multiple conversation sessions. Each session should be uniquely identifiable and associated with its own message history.\n",
        "\n",
        "#### Defining the Conversation Structure\n",
        "Create a template that includes:\n",
        "- A system message defining the AI's role\n",
        "- A placeholder for conversation history\n",
        "- The user's input\n",
        "This structure guides the AI's responses and maintains consistency throughout the conversation.\n",
        "\n",
        "#### Building the Conversational Chain\n",
        "Combine the prompt template with the language model to create a basic conversational chain. Wrap this chain with a history management component that automatically handles the insertion and retrieval of conversation history.\n",
        "\n",
        "#### Interacting with the Agent\n",
        "To use the agent, invoke it with a user input and a session identifier. The history manager takes care of retrieving the appropriate conversation history, inserting it into the prompt, and storing new messages after each interaction.\n",
        "\n",
        "### Conclusion\n",
        "This approach to creating a conversational agent offers several advantages:\n",
        "\n",
        "- Context Awareness: The agent can refer to previous parts of the conversation, leading to more natural interactions.\n",
        "- Simplicity: The modular design keeps the implementation straightforward.\n",
        "- Flexibility: It's easy to modify the conversation structure or switch to a different language model.\n",
        "- Scalability: The session-based approach allows for managing multiple independent conversations.\n",
        "With this foundation, you can further enhance the agent by:\n",
        "\n",
        "- Implementing more sophisticated prompt engineering\n",
        "- Integrating it with external knowledge bases\n",
        "- Adding specialized capabilities for specific domains\n",
        "- Incorporating error handling and conversation repair strategies\n",
        "By focusing on context management, this conversational agent design significantly improves upon basic chatbot functionality, paving the way for more engaging and helpful AI assistants.\n",
        "\n",
        "### Conversational Agent\n",
        "This notebook demonstrates how to create a simple conversational agent using LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MX6DmRo1Mvu"
      },
      "source": [
        "##### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw_gY0p8y5Ip",
        "outputId": "f0c3ee43-2111-42df-8b27-2696ed790858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2.2/2.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain huggingface_hub openai  tiktoken python-dotenv\n",
        "!pip -q install sentence_transformers langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9d1b41b",
        "outputId": "f17a358f-b5c5-4256-a53f-cd0ced6b3c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0MZO1rz918_d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUcMSXE49I6"
      },
      "source": [
        "##### Load environment variables and initalize the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f91c81b0"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(model_name = \"llama3-8b-8192\", max_tokens=1000, temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHvJAQAn5HGy"
      },
      "source": [
        "##### Create a simple in-memory store for chat histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s1Ek4lX04tUz"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "\n",
        "def get_chat_history(session_id: str):\n",
        "  if session_id not in store:\n",
        "    store[session_id] = ChatMessageHistory()\n",
        "  return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-WjpnL5mBb"
      },
      "source": [
        "##### Create the prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KnQblRD15pzL"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdzPY5i6520J"
      },
      "source": [
        "##### Combine the prompt and model into a runnable chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tL-zO0iT58mi"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nn3nLsE6A5i"
      },
      "source": [
        "##### Wrap the chain with message history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U68LlXCw6FRy"
      },
      "outputs": [],
      "source": [
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKfJC7ME6dmB"
      },
      "source": [
        "##### Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZfk7bvw6b3S",
        "outputId": "d1ee53f8-03d2-4866-fcde-608a9ef9b870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Hello! I'm doing well, thank you for asking! I'm a helpful assistant, here to assist you with any questions or tasks you may have. How about you? How's your day going so far?\n",
            "AI: Your previous message was \"Hello! How are u?\".\n"
          ]
        }
      ],
      "source": [
        "session_id = \"user_01\"\n",
        "\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hello! How are u?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"AI:\", response1.content)\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What was my previous message?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"AI:\", response2.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xRZ7nxV7b2J",
        "outputId": "09bab412-b4e4-4611-ed46-ce5e7180c13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Conversation History:\n",
            "human: Hello! How are u?\n",
            "ai: Hello! I'm doing well, thank you for asking! I'm a helpful assistant, here to assist you with any questions or tasks you may have. How about you? How's your day going so far?\n",
            "human: What was my previous message?\n",
            "ai: Your previous message was \"Hello! How are u?\".\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nConversation History:\")\n",
        "for message in store[session_id].messages:\n",
        "    print(f\"{message.type}: {message.content}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
